# Presentation Script: Aviation Incident Diagnosis Engine
## 11 Minute Word-for-Word Script

**Total Words:** ~1,259 (11:10 minutes at 150 words/min)
**Author:** Kanu Shetkar

---

## Instructions:
- Read only the "SAY THIS" sections word-for-word
- Follow screen directions at the start of each section
- Practice timing: Should be ~11 minutes total
- Don't rush - clear articulation is important

---

## **INTRODUCTION (25 seconds - 65 words)**

**üì∫ SCREEN:**
- Show README_PRESENTATION.md title

**SAY THIS:**

December 28th, 2016. American Airlines Flight 383 catches fire during takeoff at Chicago O'Hare. All passengers evacuate safely, but investigators face a challenge: searching eighty thousand historical incidents to find similar engine fire cases. This manual analysis takes hours.

I'm Kanu Shetkar. My Aviation Incident Diagnosis Engine does this search in seconds using transformer embeddings and LLM agents, demonstrating how transformers enable safety-critical applications while mitigating hallucination risks.

---

## **SECTION 1: PROBLEM STATEMENT (50 seconds - 130 words)**

**üì∫ SCREEN:**
- Scroll to "1. Problem Statement & Overview"
- Point to architecture diagram when mentioned

**SAY THIS:**

Aviation safety analysts must search the NTSB database‚Äîover eighty thousand aviation incidents since 1962‚Äîto investigate patterns. Manual analysis takes hours. Traditional keyword search fails because semantically identical descriptions use different words: "engine fire during takeoff" won't match "smoke from engine compartment on departure"‚Äîsame phenomenon, different words.

Why not just ask an LLM? LLMs hallucinate facts, generating plausible diagnoses not grounded in real data. For life-or-death aviation decisions, this is unacceptable.

My hybrid architecture delivers diagnoses in seconds, not hours. It uses transformers for semantic understanding but deterministic tools for facts. Three components: an LLM agent for orchestration, a diagnostic tool for semantic search and similarity-weighted aggregation, and the NTSB database with pre-computed embeddings.

---

## **SECTION 2: METHODOLOGY (3 minutes 25 seconds - 602 words)**

### **Part A: Transformer Embeddings (1 minute 5 seconds - 162 words)**

**üì∫ SCREEN:**
- Scroll to "2.1 Transformer Embeddings"
- Point to code snippet when mentioned
- Point to example "engine fire during takeoff"

**SAY THIS:**

Let me explain how transformer concepts enable this system.

First, transformer embeddings solve the core aviation safety problem. Traditional keyword search fails because "engine fire during takeoff" and "smoke from engine compartment on departure" share zero keywords, yet describe identical failure modes.

I use text-embedding-3-small to convert incident narratives into fifteen-hundred-thirty-six dimensional semantic coordinates. Why does this work? How did the transformer learn these semantic relationships?

During pre-training on billions of text examples, the model learned to predict masked words. To correctly predict "fire" from surrounding context, it had to learn that "smoke," "combustion," and "flames" appear in similar contexts. This forces the architecture to position semantically related words close together in vector space‚Äînot through manual rules, but through statistical patterns in language.

My query "engine fire during takeoff" now finds incidents with completely different wording‚Äî"fire in engine compartment," "smoke from engine," "combustion event during climb"‚Äîbecause they're close in semantic vector space, not keyword space. This is the fundamental advantage: transformers capture meaning, not just words.

---

### **Part B: Transformer Architecture Fundamentals (20 seconds - 50 words)**

**üì∫ SCREEN:**
- Scroll to "2.2 Transformer Architecture: Why Transfer Learning Works"
- Point to Multi-Head Attention algorithm when mentioned

**SAY THIS:**

Second, why does this transfer learning work without aviation-specific fine-tuning? Multi-head attention is key. Different attention heads specialize in different semantic dimensions‚Äîcombustion terms, aircraft components, emergency terminology. This automatic decomposition of meaning explains why semantic search achieves eighty-eight percent precision versus forty-two percent for keywords.

---

### **Part C: Attention-Inspired Similarity Search (20 seconds - 50 words)**

**üì∫ SCREEN:**
- Scroll to "2.3 Attention-Inspired Similarity"
- Point to algorithm when mentioned

**SAY THIS:**

Third, adapting attention principles for document-level similarity. Just as attention uses dot products‚ÄîQ times K-transpose‚ÄîI use cosine similarity to measure how aligned query and incident vectors are in semantic space. This computes similarity against all eighty thousand NTSB incidents, returning the top fifty ranked by relevance.

---

### **Part D: Similarity-Weighted Aggregation (1 minute 20 seconds - 190 words)**

**üì∫ SCREEN:**
- Scroll to "2.4 Similarity-Weighted Aggregation"
- Point to formula when mentioned

**SAY THIS:**

Fourth, similarity-weighted aggregation‚Äîthis is my key innovation adapting attention-style weighting to diagnostic reasoning.

Here's the problem: I have forty-three similar incidents, each with different root causes. How do I combine this evidence? Why not just count causes?

Because relevance matters more than frequency. Imagine ten incidents cite "pilot error" but they're only sixty percent similar to my query‚Äîthat's a total weight of six-point-zero. Now imagine five incidents cite "mechanical failure" but they're ninety-five percent similar‚Äîthat's a weight of four-point-seven-five.

Simple counting would say "pilot error" is more likely because ten is greater than five. But my weighted approach correctly recognizes that the five highly-similar mechanical failures are more relevant evidence than ten loosely-similar pilot errors.

Why this approach? Because similarity scores represent evidence strength. A ninety-five percent similar incident is much stronger evidence than a sixty percent similar one‚Äîit should contribute proportionally more to the probability. This treats similarity as confidence weights, just like attention mechanisms weight tokens by relevance.

The formula: weighted probability of cause j equals sum of similarities where that cause appears, divided by sum of all similarities. This ensures probabilities sum to one while respecting evidence quality.

---

### **Part E: LLM Function Calling (1 minute - 150 words)**

**üì∫ SCREEN:**
- Scroll to "2.5 LLM Function Calling"
- Point to three-step workflow diagram when mentioned

**SAY THIS:**

Finally, LLM function calling‚Äîthis orchestrates the system while preventing hallucination.

My agent has three steps. Step one: the LLM generates a detailed NTSB-style incident report from the brief query, demonstrating generation capability.

Step two: the LLM calls my diagnostic tool for factual data. This is function calling‚Äîthe LLM doesn't generate the diagnosis, it retrieves it. The tool executes the semantic search and similarity-weighted aggregation, returning statistical results from real historical data.

Step three: the LLM synthesizes these tool results into plain English. It interprets the statistics but doesn't invent facts‚Äîeverything is grounded in tool output.

Three API calls implement this: generation, tool call with diagnostic function defined, and synthesis.

This demonstrates separation of concerns: the LLM handles orchestration and explanation, the tool handles facts. The diagnosis cannot be hallucinated because it's retrieved from historical data, not generated.

---

## **SECTION 3: DEMO (1 minute - 150 words)**

**üì∫ SCREEN:**
- Switch to Streamlit app at localhost:8501
- Enter query "engine fire during takeoff" and click "Run Diagnostic Agent"
- Focus on Output 2 (diagnostic results)

**SAY THIS:**

Let me demonstrate this system in action. Query: "engine fire during takeoff."

The Streamlit interface shows three outputs. Output one: LLM-generated NTSB-style incident synopsis.

Output two‚Äîthe critical diagnostic results‚Äîshows factual data from historical analysis, not hallucinations.

The system found forty-three similar historical incidents. The top three causes each have four percent probability: maintenance personnel installation issues, engine turbine section failure, and turbine section fatigue and corrosion‚Äîeach appearing in three of the forty-three similar incidents.

Each probability uses similarity-weighted aggregation‚Äîmore similar incidents contribute more weight.

Output three: LLM synthesis interpreting the statistics in plain English.

This demonstrates the complete transformer pipeline: embeddings enabled semantic search finding forty-three relevant cases, attention-inspired similarity ranked them by relevance, and weighted aggregation calculated evidence-based probabilities. LLM orchestration with deterministic factual grounding.

---

## **SECTION 4: EVALUATION (1 minute 5 seconds - 187 words)**

**üì∫ SCREEN:**
- Switch back to README, scroll to "4. Assessment & Evaluation"
- Point to Performance Evaluation table
- Point to model architecture table
- Point to Intended Uses section
- Point to Ethical Considerations section
- Briefly show model cards tables at end

**SAY THIS:**

Let me cover evaluation and model cards.

I evaluated semantic search against keyword baseline using five test queries. Methodology: for each query, I manually reviewed top-ten results and labeled them relevant or irrelevant based on whether they described similar incident types.

Results: semantic search achieved eighty-eight-point-three percent average precision versus forty-two percent for keyword matching‚Äîa two-point-one-times improvement. This improvement was consistent across all test queries, suggesting systematic advantage rather than chance. For "engine fire during takeoff," semantic search found "smoke from engine compartment on departure"‚Äîkeyword search missed it. This validates transformer embeddings capture semantics, not just lexical matching.

Two models power this: GPT-4-o-mini for generation, orchestration, and synthesis, and text-embedding-3-small for semantic similarity with fifteen-hundred-thirty-six dimensions.

This system is appropriate for research, historical pattern analysis, and educational demonstration of hybrid AI. It's not intended for real-time flight operations or regulatory compliance without validation. It provides probabilities, not certainties‚Äîit supports expert analysis but doesn't replace it.

Ethical considerations: The historical data may reflect reporting biases. My mitigation is the weighted approach that accounts for relevance. LLM hallucinations are mitigated because the diagnostic facts come from the tool, not generation. For safety-critical domains, this is educational use only and requires expert validation.

Complete model and data cards are in the README with licenses, documentation links, and specifications.

---

## **SECTION 5: CRITICAL ANALYSIS (30 seconds - 75 words)**

**üì∫ SCREEN:**
- Scroll to "6. Critical Analysis"

**SAY THIS:**

Impact: diagnoses in seconds versus hours, grounded in historical data.

Two key insights: First, transfer learning works‚Äîtransformers trained on general text transfer to aviation without fine-tuning, capturing fundamental semantic structures.

Second, the hallucination-accuracy tradeoff. LLM generation enables synthesis but risks hallucination. Embeddings enable factual retrieval. Solution: hybrid architecture‚ÄîLLMs for orchestration, embeddings for grounding.

Limitations: small-scale evaluation. Future work: expert validation, FAISS scalability.

---

## **SECTION 6: WRAP-UP (20 seconds - 50 words)**

**üì∫ SCREEN:**
- Scroll to bottom showing Repository link
- Show GitHub URL prominently

**SAY THIS:**

In summary: transformer architectures‚Äîembeddings, attention, and function calling‚Äîenable responsible safety-critical AI applications.

Code, model cards, and documentation: https://github.com/Kshetkar1/NTSB-aviation-diagnosis-Transformers-Project.

Thank you. Questions?

---

## **END OF PRESENTATION**

**Total Word Count:** ~1,259 words
**Expected Duration:** 11:10 minutes (at 150 wpm)
**Rubric Coverage:** All 8 sections covered ‚úì
**Optimizations:** Removed incorrect "Bayesian" terminology, added mechanism depth, streamlined course review content

---

## **Timing Breakdown:**

- Introduction: 25 sec (~65 words) ‚≠ê REAL INCIDENT HOOK
- Problem Statement: 50 sec (~130 words)
- Methodology: 3:25 min (~602 words) - HIGHEST VALUE (50 pts)
  - Part A: Transformer Embeddings (1:05 - 162 words) ‚≠ê ADDED HOW TRANSFORMERS LEARN
  - Part B: Transformer Architecture Fundamentals (20 sec - 50 words) ‚≠ê STREAMLINED
  - Part C: Attention-Inspired Similarity Search (20 sec - 50 words) ‚≠ê STREAMLINED
  - Part D: Similarity-Weighted Aggregation (1:20 - 190 words) ‚≠ê ADDED WHY WEIGHTED AGGREGATION
  - Part E: LLM Function Calling (1 min - 150 words) ‚≠ê STREAMLINED
- Demo: 1 min (~150 words) ‚≠ê FOCUSED ON OUTPUT 2
- Evaluation: 1:05 min (~187 words) - HIGH VALUE (15 pts) ‚≠ê ADDED STATISTICAL FRAMING
- Critical Analysis: 30 sec (~75 words) ‚≠ê TWO KEY INSIGHTS
- Wrap-up: 20 sec (~50 words)
- **Total: ~11:10 minutes (1,259 words)**

**Note:** Optimized for 11-minute target. Improvements: (1) Removed "Bayesian" terminology (incorrect), (2) Added depth on HOW transformers learn and WHY weighted aggregation, (3) Streamlined course review content to focus on novel contributions.
